---
title: "論文閱讀 Explanation-Guided Backdoor Poisoning Attacks Against Malware Classifiers"
date: 2022-02-26T13:39:01+08:00
draft: false
toc: true
images:
tags: 
  - 論文
---

## Introduction

(i) 為了取得多元、大量的樣本，相關企業允許使用者上傳資料，作為訓練模型的依據，這可能成為攻擊的目標。

(ii) 提出如何在不針對特定模型的 feature-based 分類器上製造後門的方法。

(iii) explanation-guided backdoor attacks 如何實際應用在 PE 檔案，以及安卓跟 PDF 檔。

(iv) 如何防範此類攻擊。

## Background
### Malware Detection Systems
* dynamic analysis: 在虛擬環境運行 binary file，紀錄它的活動行為來分析。
* static analysis: 不會執行 file，而是直接從 binary 提取特徵來分析。又可細分為 feature-based detectors 與 raw-binary analyzers。

這篇是用 static analysis，考量到它能在執行前偵測。

### Adversarial Attacks
* evasion attacks: 透過在測試資料上面加上細微的擾動，造成錯誤的分類。
* poisoning attacks: 透過在訓練資料加入或修改 data，造成錯誤的分類。

其中 poisoning attacks 還可再分為：
* Availability poisoning attacks: 讓模型整體正確率下降
* Targeted poisoning: 造成在特定類別的錯誤分類
* Back-door attacks: 特定的 feature 與 value 組合，讓 model 把這個 pattern 跟 target class 聯繫起來

Clean-labelvariants of the attacks: 不需要操縱被汙染 data 的 label 的攻擊手法。

### SHapley Additive exPlanations

SHAP 值用來表示特定 feature 對模型做出判斷的貢獻程度(重要性)，以及讓模型分類到哪種 class (方向性)。

## Problem Statement and Threat Model
![](https://i.imgur.com/n28zQjD.png)

第三方平台提供使用者(包含攻擊者)上傳檔案，運作的反毒軟體會將檔案分類。如果是靜態分析，在訓練前，還會從 PE file 提取特徵。接著模型就會公開部署，對新的 binary file 做分類(benign or malware)。由於攻擊者並不能直接改動這些 label，就需要用 clean-label 的 backdoor。

攻擊者的目標是在 benign 檔案中挾帶後門，同時散佈到標記平台，連帶汙染下游的惡意軟體分類器。一旦模型部署後，攻擊者就可以用相同的後門浮水印來測試，看是否能攻擊成功。

### Threat Model
BadNet 定義：

(1) “Outsourced Training Attack”: 使用者只能用提供的資料集來驗證結果

(2) “Transfer Learning Attack”: 使用者自己下載預訓練模型並做調整

* 目標

對後門攻擊來說，攻擊者的目標是當輸入不含後門的 input 時，要做出正確的分類；當輸入含有後門的 input 則會變成特定的結果。
![](https://i.imgur.com/9ypKmwi.png)

在這個情況中，預測結果不是 benign (0) 就是 malicious (1)，並且攻擊者希望的是將原本是 malware 的被斷成 benign。為了讓攻擊不容易被發現，也會希望讓 trigger 的大小跟污染的資料集數量越小越好。

* 情況假設

為了限縮製作浮水印的可能性，而非所有的數值都有可能，假設攻擊者只會把值改成有出現在 benign sample 中的。

四種可能的假設：

(1) unrestricted

(2) data_limited: 無法存取到所有 training data

(3) transfer: 不知道 target model 情況

(4) black_box: 不清楚 target model 的架構

(5) constrained: 只能修改特定的 feature 與 value

## Explanation-Guided Backdoor Attacks

後門攻擊會創造一個高密度的 feature subspace，讓分類器因此調整決策邊界。只需要一個相對小的 subspace，若它的密度很高或者處在模型信心較低的地方 ，依舊可以影響決策邊界。

製造後門的兩種策略：
(1) 找決策邊界附近，信心最弱的地方
(2) 找非常偏向 goodware 的區域覆蓋過去

因此，接著出現的問題是：要怎麼找到模型的決策方式？這篇文章用 SHAP 作為指標。若 SHAP 值為負，代表會讓模型認為是 benign ；若為正則認為是 malware。對單一樣本來說，對所有 feature 的 SHAP 值作加總，其意義相當於模型的 output。

若想要找到模型決策邊界信心程度較低的區域，可以透過 SHAP 值找接近 0 的地方。

### Building Blocks
* Feature Selection: 找最 importance 的 feature
    * LargeSHAP: 把個別的 SHAP 值做加總，找到 class 的平均值。由此可知在特定 class 下，feature 的重要性。如果越負代表對判斷 goodware 重要，反之代表對 malware 重要。接近於零，表示不靠近任何的類別而且信心程度較弱。
    * LargeAbsSHAP: 不管方向性(benign 還 malware)，用先用絕對值再作加總。結果會知道 feature 的重要性，但不管偏向哪種類別。
    
* Value Selection: 
    * MinPopulation: 找 dataset 中出現頻率最小的 value。
    * CountSHAP: 找出現頻率高而且 goodware-oriented 的 區域。
    ![](https://i.imgur.com/kMQXKqc.png)
    * CountAbsSHAP:
    ![](https://i.imgur.com/8WRSfMz.png)

### Attack Strategies
* Independent Selection: 個別找 feature 跟 value 的組合，屬於找決策邊界附近信心較弱的區域。
* Greedy Combined Selection: 透過貪婪演算法找 goodware-oriented points 中最適合的 feature 跟 value，跟那些 points 融合在一起。不斷移除不屬於所選 trigger 的 data，重複多次後剩下來的就是 trigger。
![](https://i.imgur.com/BcMzBDv.png)


兩種方法差在前者是直接找幾個 feature，接著做 value selection；後者是找出 goodware-oriented points 中共有的那些 subspace 作為 trigger。前者是找密度比較低的地方，後者找的是符合語意的 pattern。
## Experimental Attack Evaluation

EMBER (PE 型態的資料集) + LightGBM/EmberNN

三種指標：Acc(Fb, Xb), Acc(Fb, X), FPb

### Attack Performance
### Limiting the Attacker

##  Problem-Space Considerations
### Windows PEs
### Other Datasets

## Mitigation